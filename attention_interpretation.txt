======================================================================
ATTENTION PATTERN INTERPRETATION
======================================================================

1. TEMPORAL FOCUS DISTRIBUTION
   • Recent past (1-7 days): 36.55% of top attention
   • Medium past (8-15 days): 7.42% of top attention
   • Distant past (16+ days): 23.03% of top attention

2. KEY FINDINGS
   • The model attends to LONG-TERM patterns (2+ weeks)
   • This reveals seasonal or monthly dependencies

3. TOP ATTENDED TIME LAGS
   1. Lag 0: 0.3341 (49.9% of top-10)
      → immediate past (yesterday)
   2. Lag 15: 0.0463 (6.9% of top-10)
      → three weeks ago
   3. Lag 16: 0.0449 (6.7% of top-10)
      → three weeks ago
   4. Lag 17: 0.0436 (6.5% of top-10)
      → three weeks ago
   5. Lag 14: 0.0406 (6.1% of top-10)
      → medium past (~14 days ago, bi-weekly)

4. PRACTICAL INSIGHTS
   The attention mechanism has learned to:
   • Prioritize recent trends for short-term forecasting
   • Capture weekly and bi-weekly patterns

   This selective attention allows the model to:
   • Focus on the MOST RELEVANT historical data
   • Ignore noise and less important time periods
   • Adapt predictions based on temporal context

======================================================================